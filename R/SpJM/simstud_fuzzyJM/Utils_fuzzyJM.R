library(cluster)
library(StatMatch)
library(poliscidata) # for weighted mode 
library(pdfCluster) # for ARI
library(parallel)
library(foreach)
library(doParallel)

weighted_median <- function(x, weights) {
  # Ensure x and weights are of the same length
  if (length(x) != length(weights)) {
    stop("x and weights must have the same length.")
  }
  
  # Sort x and weights by x
  sorted_indices <- order(x)
  x <- x[sorted_indices]
  weights <- weights[sorted_indices]
  
  # Compute cumulative weights
  cumulative_weights <- cumsum(weights)
  total_weight <- sum(weights)
  
  # Find the smallest x such that the cumulative weight is >= 50% of the total weight
  weighted_median <- x[which(cumulative_weights >= total_weight / 2)[1]]
  
  return(weighted_median)
}


weighted_mode <- function(x, weights) {
  # Ensure x and weights are of the same length
  if (length(x) != length(weights)) {
    stop("x and weights must have the same length.")
  }
  
  # Aggregate weights for each unique value of x
  unique_x <- unique(x)
  aggregated_weights <- sapply(unique_x, function(val) sum(weights[x == val]))
  
  # Find the value of x with the maximum aggregated weight
  weighted_mode <- unique_x[which.max(aggregated_weights)]
  
  return(weighted_mode)
}


Mode <- function(x,na.rm=T) {
  if(na.rm){
    x <- x[!is.na(x)]
  }
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
  
}

get_cat=function(y,mc,mu_val,phi){
  # Function to simulate categorical data
  
  # Arguments:
  # y: continuous variable 
  # mc: Markov chain states
  # mu: numeric mean value
  # phi: conditional probability for the categorical outcome k in state k
  
  library(dplyr)
  
  mu_val=c(-mu_val,0,mu_val)
  #K=length(unique(mc))
  #mu=seq(-mu,mu,length.out=K)
  #phi1=(1-phi)/(K-1)
  phi1=(1-phi)/2
  
  TT=length(y)
  for(i in 1:TT){
    k=mc[i]
    switch(k,
           "1"={
             threshold=c(qnorm(phi1,mu_val[1]),qnorm(phi+phi1,mu_val[1]))
             if(y[i]>threshold[1]&y[i]<threshold[2]){
               y[i]=1
             }
             else if(y[i]<threshold[1]){
               y[i]=2
             }
             else{
               y[i]=3
             }
           },
           "2"={
             threshold=c(qnorm(phi1,mu_val[2]),qnorm(phi+phi1,mu_val[2]))
             if(y[i]>threshold[1]&y[i]<threshold[2]){
               y[i]=2
             }
             else if(y[i]<threshold[1]){
               y[i]=3
             }
             else{
               y[i]=1
             }
           },
           "3"={
             threshold=c(qnorm(phi1,mu_val[3]),qnorm(phi+phi1,mu_val[3]))
             if(y[i]>threshold[1]&y[i]<threshold[2]){
               y[i]=3
             }
             else if(y[i]<threshold[1]){
               y[i]=1
             }
             else{
               y[i]=2
             }
           }
    )
  }
  return(y)
  
}

order_states_condMean=function(y,s){
  
  # This function organizes states by assigning 1 to the state with the smallest conditional mean for vector y
  # and sequentially numbering each new state as 2, 3, etc., incrementing by 1 for each newly observed state.
  
  #Slong=c(t(S))
  # condMeans=sort(tapply(y,Slong,mean,na.rm=T))
  condMeans=sort(tapply(y,s,mean,na.rm=T))
  
  states_temp=match(s,names(condMeans))
  
  #states_temp=matrix(states_temp,nrow=nrow(S),byrow = T)
  
  return(states_temp)
}

order_states_condMed=function(y,s){
  
  # This function organizes states by assigning 1 to the state with the smallest conditional median for vector y
  # and sequentially numbering each new state as 2, 3, etc., incrementing by 1 for each newly observed state.
  
  #Slong=c(t(S))
  # condMeans=sort(tapply(y,Slong,mean,na.rm=T))
  condMed=sort(tapply(y,s,median,na.rm=T))
  
  states_temp=match(s,names(condMed))
  
  #states_temp=matrix(states_temp,nrow=nrow(S),byrow = T)
  
  return(states_temp)
}

punct=function(x,pNAs,typeNA){
  
  # x is a vector (column of the dataset)
  # pNAs is the percentage of missing values
  # typeNA is the type of missing values (0 for random, 1 for continuous, all other values will turn into no missing imputation)
  
  TT=length(x)
  pTT=round(TT*pNAs)
  if(typeNA==0){
    NAindx=sample(1:TT,pTT,replace = F)
    x[NAindx]=NA
  }
  else if(typeNA==1){
    NAindx=sample(1:(TT-pTT),1,replace = F)
    NAindx=seq(NAindx,NAindx+pTT)
    x[NAindx]=NA
  }
  
  return(x)
  
}

initialize_states <- function(Y, K) {
  n <- nrow(Y)
  
  ### Repeat the following few times?
  centr_indx=sample(1:n, 1)
  centroids <- Y[centr_indx, , drop = FALSE]  # Seleziona il primo centroide a caso
  
  closest_dist <- as.matrix(daisy(Y, metric = "gower"))
  closest_dist <- closest_dist[centr_indx,]
  
  for (i in 2:K) {
    prob <- closest_dist / sum(closest_dist)
    next_centr_indx <- sample(1:n, 1, prob = prob)
    next_centroid <- Y[next_centr_indx, , drop = FALSE]
    centroids <- rbind(centroids, next_centroid)
  }
  ###
  
  # init_stats=rep(0,n)
  # For cycle
  # for(i in 1:n){
  #   init_stats[i]=which.min(gower.dist(Y[i,],centroids))
  # }
  
  # Using sapply and vapply
  # init_stats2 <- sapply(1:n, function(i) which.min(gower.dist(Y[i,], centroids)))
  # init_stats3 <- vapply(1:n, function(i) which.min(gower.dist(Y[i,], centroids)), integer(1))
  
  # Faster solution 
  dist_matrix <- gower.dist(Y, centroids)
  init_stats <- apply(dist_matrix, 1, which.min)
  
  return(init_stats)
}

# Temporal ----------------------------------------------------------------


sim_data_mixed=function(seed=123,
                        TT,
                        P,
                        Ktrue=3,
                        mu_val=1,
                        phi=.8,
                        rho=0,
                        Pcat=NULL,
                        pers=.95,
                        pNAs=0,
                        typeNA=3){
  
  # Function to simulate mixed data with fixed parameters for the data generating process
  
  # Arguments:
  # seed: seed for the random number generator
  # TT: number of observations
  # P: number of features
  # Ktrue: number of states
  # mu: mean value for the continuous variables
  # phi: conditional probability for the categorical outcome k in state k
  # rho: correlation for the variables
  # Pcat: number of categorical variables
  # pers: self-transition probability
  # pNAs: percentage of missing values
  # typeNA is the type of missing values (0 for random, 1 for continuous, all other values will turn into no missing imputation)
  
  # value:
  # SimData: matrix of simulated data
  
  MU=mu_val
  mu_val=c(-mu_val,0,mu_val)
  
  if(is.null(Pcat)){
    Pcat=floor(P/2)
  }
  
  # Markov chain simulation
  x <- numeric(TT)
  Q <- matrix(rep((1-pers)/(Ktrue-1),Ktrue*Ktrue), 
              ncol = Ktrue,
              byrow = TRUE)
  diag(Q)=rep(pers,Ktrue)
  init <- rep(1/Ktrue,Ktrue)
  set.seed(seed)
  x[1] <- sample(1:Ktrue, 1, prob = init)
  for(i in 2:TT){
    x[i] <- sample(1:Ktrue, 1, prob = Q[x[i - 1], ])
  }
  
  # Continuous variables simulation
  Sigma <- matrix(rho,ncol=P,nrow=P)
  diag(Sigma)=1
  
  Sim = matrix(0, TT, P * Ktrue)
  SimData = matrix(0, TT, P)
  
  set.seed(seed)
  for(k in 1:Ktrue){
    u = MASS::mvrnorm(TT,rep(mu_val[k],P),Sigma)
    Sim[, (P * k - P + 1):(k * P)] = u
  }
  
  for (i in 1:TT) {
    k = x[i]
    SimData[i, ] = Sim[i, (P * k - P + 1):(P * k)]
    #SimDataCat[i, ] = SimCat[i, (Pcat * k - Pcat + 1):(Pcat * k)]
  }
  
  if(Pcat!=0){
    SimData[,1:Pcat]=apply(SimData[,1:Pcat],2,get_cat,mc=x,mu_val=MU,phi=phi)
    SimData=as.data.frame(SimData)
    SimData[,1:Pcat]=SimData[,1:Pcat]%>%mutate_all(as.factor)
  }
  
  if(typeNA==0|typeNA==1){
    SimData.NA=apply(SimData,2,punct,pNAs=pNAs,type=typeNA)
    SimData.NA=as.data.frame(SimData.NA)
    SimData.NA[,1:Pcat]=SimData.NA[,1:Pcat]%>%mutate_all(as.factor)
    SimData.NA[,-(1:Pcat)]=SimData.NA[,-(1:Pcat)]%>%mutate_all(as.numeric)
  }
  else{
    SimData.NA=SimData
  }
  
  return(list(
    SimData.NA=SimData.NA,
    SimData.complete=SimData,
    mchain=x,
    TT=TT,
    P=P,
    Ktrue=Ktrue,
    pers=pers, 
    seed=seed,
    P=P,
    Pcat=Pcat))
  
}


fuzzy_jump <- function(Y, 
                       n_states, jump_penalty=1e-5, 
                       initial_states=NULL,
                       max_iter=10, n_init=10, tol=NULL, 
                       verbose=FALSE
                       # ,
                       # alpha=NULL
                       # # ,
                       #        time_vec=NULL
                       
) {
  # Fit jump model for mixed type data 
  
  # Arguments:
  # Y: data.frame with mixed data types. Categorical variables must be factors.
  # n_states: number of states
  # jump_penalty: penalty for the number of jumps
  # initial_states: initial state sequence
  # max_iter: maximum number of iterations
  # n_init: number of initializations
  # tol: tolerance for convergence
  # verbose: print progress
  # time_vec is a vector of time points, needed if times are not equally sampled
  
  # Value:
  # best_s: estimated state sequence
  # Y: imputed data
  # Y.orig: original data
  # condMM: state-conditional medians and modes
  
  # timeflag=FALSE
  # if(!is.null(time_vec)){
  #   timeflag=TRUE
  #   if(length(time_vec)!=nrow(Y)){
  #     stop("time_vec must have the same length of the number of observations")
  #   }
  #   else{
  #     time=sort(unique(time_vec))
  #     dtime=diff(time)
  #     dtime=dtime/as.numeric(min(dtime))
  #     dtime=as.numeric(dtime)
  #   }
  # }
  
  n_states=as.integer(n_states)
  
  n_obs <- nrow(Y)
  n_features <- ncol(Y)
  best_loss <- NULL
  best_S <- NULL
  
  # Which vars are categorical and which are numeric
  cat_flag=any(sapply(Y, is.factor))
  
  if(cat_flag){
    cat.indx=which(sapply(Y, is.factor))
    cont.indx=which(sapply(Y, is.numeric))
    Ycont=Y[,cont.indx]
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    Ycat=Y[,cat.indx]
    
    if(length(cat.indx)==1){
      n_levs=length(levels(Ycat))
    }
    else{
      n_levs=apply(Ycat, 2, function(x)length(unique(x[!is.na(x)])))
    }

    n_cat=length(cat.indx)
    n_cont=n_features-n_cat
    
  }
  else{
    cont.indx=1:n_features
    Ycont=Y
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    n_cont=dim(Y)[2]
    n_cat=0
  }
  
  
  
  
  for (init in 1:n_init) {
    
    # State initialization through kmeans++
    if (!is.null(initial_states)) {
      s <- initial_states
    } else {
      s=initialize_states(Y,n_states)
    }
    
    S <- matrix(0, nrow = n_obs, ncol = n_states)
    row_indices <- rep(1:n_obs)  # Row positions in SS
    # Assign 1s in a single step
    S[cbind(row_indices, s)] <- 1 
    
    mu <- matrix(0, nrow=n_states, ncol=length(cont.indx))
    if(cat_flag){
      mo <- matrix(0, nrow=n_states, ncol=length(cat.indx))
    }
    
    for (i in unique(s)) {
      # substitute with medians
      mu[i,] <- apply(Ycont[s==i,], 2, median, na.rm = TRUE)
      if(cat_flag){
        if(length(cat.indx)==1){
            mo[i,]=Mode(Ycat[s==i])
        }
        else{
          mo[i,]=apply(Ycat[s==i,],2,Mode)
        }
      }
    }
    
    mu=data.frame(mu)
    mumo=data.frame(matrix(0,nrow=n_states,ncol=n_features))
    
    if(cat_flag){
      mo=data.frame(mo,stringsAsFactors=TRUE)
      for(i in 1:n_cat){
        if(length(cat.indx)==1){
          mo[,i]=factor(mo[,i],levels=levels(Ycat[i]))
        }
        else{
          mo[,i]=factor(mo[,i],levels=levels(Ycat[,i]))
        }
      }
      mumo=data.frame(matrix(0,nrow=n_states,ncol=n_features))
      mumo[,cat.indx]=mo
    }
    
    mumo[,cont.indx]=mu
    colnames(mumo)=colnames(Y)
    
    S_old=S
    loss_old <- 1e10
    for (it in 1:max_iter) {
      

      # E step
      V=gower.dist(Y,mumo)^2
      V1=1/V
      V1_lambda=1/(V+jump_penalty)
      S_til=V1_lambda/rowSums(V1_lambda)
      S=matrix(0,nrow=n_obs,ncol=n_states)
      # FUZZY
      S[1,]=V1[1,]/sum(V1[1,])
      for(t in 2:n_obs){
        S[t,]=S_til[t,]-
          jump_penalty*sum(S[(t-1),]/(V[t,]+jump_penalty))*S_til[t,]+
          jump_penalty*S[(t-1),]/(V[t,]+jump_penalty)
        
      }
      # S[n_obs,]=V1[n_obs,]/sum(V1[n_obs,])
      # for(t in (n_obs-1):1){
      #   
      #   # beta=(2*(1-jump_penalty*sum(S[(t+1),]/(V[t,]+jump_penalty))))/
      #   #   (sum(V1_lambda[t,]))
      #   # S[t,]=(beta+2*jump_penalty*S[(t+1),])/(2*V[t,]+2*jump_penalty)
      #   
      #   S[t,]=S_til[t,]-
      #     jump_penalty*sum(S[(t+1),]/(V[t,]+jump_penalty))*S_til[t,]+
      #     jump_penalty*S[(t+1),]/(V[t,]+jump_penalty)
      #   
      # }
      
      #S=t(apply(S,1,function(x) x/sum(x)))
      
      # But this is not the true loss
      #loss <- min(V[1,])
      
      S_prec=rbind(rep(0,n_states),S[-n_obs,])
      Lambda=jump_penalty*(S-S_prec)^2
      Lambda[1,]=0
      loss=sum(S^2*V+Lambda)
      
      # M step
      for(k in 1:n_states){
        #mu[k,]=apply(Ycont, 2, function(x) weighted_median(x, weights = S[,k]))
        #mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k])})
        mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k]^2)})
        if(cat_flag){
          if(n_cat==1){
            #mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k])
            mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k]^2)
            
          }
          else{
            #mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k])})
            mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k]^2)})
            
          }
          #mo[k,]=apply(Ycat,2,function(x)weighted_mode(x,weights=S[,k]))
        }
      }
      
      if(cat_flag){
        mumo[,cat.indx]=mo
      }
      
      mumo[,cont.indx]=mu
      colnames(mumo)=colnames(Y)
      
      
      if (verbose) {
        cat(sprintf('Iteration %d: %.6e\n', it, loss))
      }
      if (!is.null(tol)) {
        epsilon <- loss_old - loss
        if (epsilon < tol) {
          break
        }
      } else if (all(S == S_old)) {
        break
      }
      loss_old <- loss
      S_old=S
    }
    if (is.null(best_S) || (loss_old < best_loss)) {
      best_loss <- loss_old
      best_S <- S
    }
    s=initialize_states(Y,n_states)
  }
  
  MAP=apply(best_S,1,which.max)
  res_Y=data.frame(Y,MAP=MAP)
  col_sort=as.integer(names(sort(tapply(res_Y[,cont.indx[1]],res_Y$MAP,mean))))
  mumo=mumo[col_sort,]
  best_S=best_S[,col_sort]
  
  return(list(best_S=best_S,
              MAP=MAP,
              Y=Y,
              condMM=mumo))
}

# objective_function <- function(s, g_values, lambda, s_t_prev,m) {
#   #sum(s^m * g_values^2) + lambda * sum((s_t_prev - s)^2)
#   sum(s^m * g_values) + lambda * sum((s_t_prev - s)^2)
# }
# 
# gradient_function <- function(s, g_values, lambda, s_t_prev, m) {
#   #m * s^(m-1) * g_values^2 - 2 * lambda * (s_t_prev - s)
#   m * s^(m-1) * g_values - 2 * lambda * (s_t_prev - s)
# }

######
get_cat_t <- function(y, mc, mu_val, phi, df = 5) {
  # y: vettore continuo (una variabile)
  # mc: stati latenti (valori da 1 a K)
  # mu_val: vettore di K valori medi (es. c(-mu, 0, mu))
  # phi: probabilità condizionata
  # df: gradi di libertà della t-Student
  
  TT <- length(y)
  K <- length(mu_val)
  phi1 <- (1 - phi) / (K - 1)  # non serve direttamente, solo se vuoi usare probabilità
  
  for (i in 1:TT) {
    k <- mc[i]
    mu_k <- mu_val[k]
    
    # Calcola l'intervallo centrato in mu_k che contiene prob = phi sotto la t-Student
    half_width <- qt((1 + phi) / 2, df = df)
    lower <- mu_k - half_width
    upper <- mu_k + half_width
    
    # Verifica se y[i] cade nell'intervallo
    if (y[i] >= lower && y[i] <= upper) {
      y[i] <- k  # corretto: valore coerente con lo stato
    } else {
      # Scegli casualmente un altro k tra gli altri K-1
      y[i] <- sample(setdiff(1:K, k), 1)
    }
  }
  return(y)
}

sim_data_stud_t=function(seed=123,
                         TT,
                         P,
                         Pcat,
                         Ktrue=3,
                         mu=1.5,
                         rho=0,
                         nu=4,
                         phi=.8,
                         pers=.95){
  
  
  MU=seq(mu, -mu, length.out=Ktrue)
  
  # Markov chain simulation
  x <- numeric(TT)
  Q <- matrix(rep((1-pers)/(Ktrue-1),Ktrue*Ktrue), 
              ncol = Ktrue,
              byrow = TRUE)
  diag(Q)=rep(pers,Ktrue)
  init <- rep(1/Ktrue,Ktrue)
  set.seed(seed)
  x[1] <- sample(1:Ktrue, 1, prob = init)
  for(i in 2:TT){
    x[i] <- sample(1:Ktrue, 1, prob = Q[x[i - 1], ])
  }
  
  # Continuous variables simulation
  Sigma <- matrix(rho,ncol=P,nrow=P)
  diag(Sigma)=1
  
  Sim = matrix(0, TT, P * Ktrue)
  SimData = matrix(0, TT, P)
  
  set.seed(seed)
  for(k in 1:Ktrue){
    # u = MASS::mvrnorm(TT,rep(mu[k],P),Sigma)
    u = mvtnorm::rmvt(TT, sigma = (nu-2)*Sigma/nu, df = nu, delta = rep(MU[k],P))
    Sim[, (P * k - P + 1):(k * P)] = u
  }
  
  for (i in 1:TT) {
    k = x[i]
    SimData[i, ] = Sim[i, (P * k - P + 1):(P * k)]
    #SimDataCat[i, ] = SimCat[i, (Pcat * k - Pcat + 1):(Pcat * k)]
  }
  
  SimData=data.frame(SimData)
  
  if(!is.null(Pcat)){
    for (j in 1:Pcat) {
      SimData[, j] <- get_cat_t(SimData[, j], x, MU, phi=phi, df = nu)
      SimData[, j]=factor(SimData[, j],levels=1:Ktrue)
    }  
  }
  
  
  
  return(list(
    SimData=SimData,
    mchain=x,
    TT=TT,
    P=P,
    K=Ktrue,
    Ktrue=Ktrue,
    pers=pers, 
    seed=seed))
  
}

objective_function <- function(s, g_values, lambda, s_t_prec,s_t_succ,m) {
  #sum(s^m * g_values) + lambda * sum((s_t_prec - s)^2+(s_t_succ - s)^2) 
  sum(s^m * g_values) + lambda *( sum(abs(s_t_prec - s))^2+sum(abs(s_t_succ - s))^2) 
}

objective_function_1T <- function(s, g_values, lambda, s_t_1,m) {
  #sum(s^m * g_values) + lambda * sum((s_t_1 - s)^2) 
  sum(s^m * g_values) + lambda * sum(abs(s_t_1 - s))^2 
}

objective_function_m=function(m,S,g){
  sum(S^m*g)
}

# optimize_fuzzifier <- function(s, g, m_range = c(1.01, 5)) {
#   # Clip s to avoid log(0)
#   s <- pmax(s, 1e-8)
#   
#   # Objective function: f(m)
#   f_m <- function(m) {
#     sum(s^m * g)
#   }
#   
#   # Derivative: f'(m), optional if using gradient-based methods
#   # grad_f_m <- function(m) {
#   #   sum(log(s) * s^m * g)
#   # }
#   
#   # Use 1D minimization over m ∈ [1.01, 5]
#   result <- optimize(f_m, interval = m_range)
#   return(result$minimum)
# }

#####

gradient_function <- function(s, g_values, lambda, s_t_prev, m) {
  #m * s^(m-1) * g_values^2 - 2 * lambda * (s_t_prev - s)
  m * s^(m-1) * g_values - 2 * lambda * (s_t_prev - s)
}

optimize_s <- function(g_values, lambda, s_t_prev, m, max_attempts = 10) {
  K <- length(g_values)
  
  attempt <- 1
  while (attempt <= max_attempts) {
    # Random initialization ensuring sum(s) = 1
    s_init <- runif(K)
    s_init <- s_init / sum(s_init)
    
    # Try optimizing
    result <- tryCatch(
      {
        nloptr::nloptr(
          x0 = s_init,
          eval_f = function(s) objective_function(s, g_values, lambda, s_t_prev, m),
          eval_grad_f = function(s) gradient_function(s, g_values, lambda, s_t_prev, m),
          lb = rep(0, K), 
          ub = rep(1, K),
          eval_g_eq = function(s) sum(s) - 1,  # Constraint: sum(s) = 1
          eval_jac_g_eq = function(s) rep(1, K),  # Jacobian of equality constraint
          opts = list("algorithm"="NLOPT_LD_SLSQP", "xtol_rel"=1e-6)
        )
      },
      error = function(e) NULL  # If error occurs, return NULL
    )
    
    # Check if optimization was successful
    if (!is.null(result) && !is.null(result$solution) && all(!is.na(result$solution))) {
      return(result$solution)  # Return solution if valid
    }
    
    attempt <- attempt + 1
  }
  
  warning("Optimization failed after ", max_attempts, " attempts. Returning uniform distribution.")
  return(rep(1/K, K))  # Return uniform distribution as a last resort
}

# optimize_s <- function(g_values, lambda, s_t_prev, m) {
#   K <- length(g_values)
#   #s_init <- rep(1/K, K)  # Initial guess (uniform)
#   s_init=runif(K)
#   s_init=s_init/sum(s_init)
#   
#   result <- nloptr::nloptr(
#     x0 = s_init,
#     eval_f = function(s) objective_function(s, g_values, lambda, s_t_prev, m),
#     eval_grad_f = function(s) gradient_function(s, g_values, lambda, s_t_prev, m),
#     lb = rep(0, K), 
#     ub = rep(1, K),
#     eval_g_eq = function(s) sum(s) - 1,  # Constraint: sum(s) = 1
#     eval_jac_g_eq = function(s) rep(1, K),  # Jacobian of equality constraint
#     opts = list("algorithm"="NLOPT_LD_SLSQP", "xtol_rel"=1e-6)
#   )
#   
#   return(result$solution)
# }



fuzzy_jump_m <- function(Y, 
                         K, lambda=1e-5, 
                         m=2,
                         initial_states=NULL,
                         max_iter=10, n_init=10, tol=NULL, 
                         verbose=FALSE
                         # ,
                         # alpha=NULL
                         # # ,
                         #        time_vec=NULL
                         
) {
  # Fit jump model for mixed type data 
  
  # Arguments:
  # Y: data.frame with mixed data types. Categorical variables must be factors.
  # K: number of states
  # lambda: penalty for the number of jumps
  # initial_states: initial state sequence
  # max_iter: maximum number of iterations
  # n_init: number of initializations
  # tol: tolerance for convergence
  # verbose: print progress
  # time_vec is a vector of time points, needed if times are not equally sampled
  
  # Value:
  # best_s: estimated state sequence
  # Y: imputed data
  # Y.orig: original data
  # condMM: state-conditional medians and modes
  
  # timeflag=FALSE
  # if(!is.null(time_vec)){
  #   timeflag=TRUE
  #   if(length(time_vec)!=nrow(Y)){
  #     stop("time_vec must have the same length of the number of observations")
  #   }
  #   else{
  #     time=sort(unique(time_vec))
  #     dtime=diff(time)
  #     dtime=dtime/as.numeric(min(dtime))
  #     dtime=as.numeric(dtime)
  #   }
  # }
  
  K=as.integer(K)
  
  TT <- nrow(Y)
  P <- ncol(Y)
  best_loss <- NULL
  best_S <- NULL
  
  # Which vars are categorical and which are numeric
  cat_flag=any(sapply(Y, is.factor))
  
  if(cat_flag){
    cat.indx=which(sapply(Y, is.factor))
    cont.indx=which(sapply(Y, is.numeric))
    Ycont=Y[,cont.indx]
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    Ycat=Y[,cat.indx]
    
    if(length(cat.indx)==1){
      n_levs=length(levels(Ycat))
    }
    else{
      n_levs=apply(Ycat, 2, function(x)length(unique(x[!is.na(x)])))
    }
    
    n_cat=length(cat.indx)
    n_cont=P-n_cat
    
  }
  else{
    cont.indx=1:P
    Ycont=Y
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    n_cont=dim(Y)[2]
    n_cat=0
  }
  
  
  
  
  for (init in 1:n_init) {
    
    # State initialization through kmeans++
    if (!is.null(initial_states)) {
      s <- initial_states
    } else {
      s=initialize_states(Y,K)
    }
    
    S <- matrix(0, nrow = TT, ncol = K)
    row_indices <- rep(1:TT)  # Row positions in SS
    # Assign 1s in a single step
    S[cbind(row_indices, s)] <- 1 
    
    mu <- matrix(NA, nrow=K, ncol=length(cont.indx))
    if(cat_flag){
      mo <- matrix(NA, nrow=K, ncol=length(cat.indx))
    }
    # us=unique(s)
    # tab_s=as.vector(table(s))
    # for (i in unique(s)) {
    #   # substitute with medians
    #   mu[i,] <- apply(Ycont[s==i,], 2, median, na.rm = TRUE)
    #   if(cat_flag){
    #     if(length(cat.indx)==1){
    #       mo[i,]=Mode(Ycat[s==i])
    #     }
    #     else{
    #       mo[i,]=apply(Ycat[s==i,],2,Mode)
    #     }
    #   }
    # }
    for (i in unique(s)) {
      # Ensure that Ycont[s == i, ] remains a matrix
      subset_Ycont <- Ycont[s == i, , drop = FALSE]
      
      # Substitute with medians, ensuring it works for a single row
      if (nrow(subset_Ycont) > 1) {
        mu[i, ] <- apply(subset_Ycont, 2, median, na.rm = TRUE)
      } else {
        mu[i, ] <- as.vector(subset_Ycont)  # Direct assignment for single row
      }
      
      if (cat_flag) {
        subset_Ycat <- Ycat[s == i, , drop = FALSE]  # Ensure it's a matrix
        
        if (length(cat.indx) == 1) {
          mo[i, ] <- Mode(subset_Ycat)
        } else {
          if (nrow(subset_Ycat) > 1) {
            mo[i, ] <- apply(subset_Ycat, 2, Mode)
          } else {
            mo[i, ] <- as.vector(subset_Ycat)  # Direct assignment for single row
          }
        }
      }
    }
    
    
    mu=data.frame(mu)
    mumo=data.frame(matrix(0,nrow=K,ncol=P))
    
    if(cat_flag){
      mo=data.frame(mo,stringsAsFactors=TRUE)
      for(i in 1:n_cat){
        if(length(cat.indx)==1){
          mo[,i]=factor(mo[,i],levels=levels(Ycat[i]))
        }
        else{
          mo[,i]=factor(mo[,i],levels=levels(Ycat[,i]))
        }
      }
      mumo=data.frame(matrix(0,nrow=K,ncol=P))
      mumo[,cat.indx]=mo
    }
    
    mumo[,cont.indx]=mu
    colnames(mumo)=colnames(Y)
    
    S_old=S
    loss_old <- 1e10
    for (it in 1:max_iter) {
      
      
      # E step
      V=gower.dist(Y,mumo)
      V1=1/V
      S=matrix(0,nrow=TT,ncol=K)
      # FUZZY
      S[1,]=(V1[1,])^(1/(m-1))/sum((V1[1,])^(1/(m-1)))
      #S[1,]=(V1[1,])^(2/(m-1))/sum((V1[1,])^(2/(m-1)))
      for(t in 2:TT){
        S[t,]=optimize_s(g_values=V[t,], 
                         lambda=lambda, 
                         s_t_prev=S[t-1,],m=m)
        
      }
      #matplot(S,type='l',main="m=1.01")
      
      #S=t(apply(S,1,function(x) x/sum(x)))
      
      # But this is not the true loss
      #loss <- min(V[1,])
      
      S_prec=rbind(rep(0,K),S[-TT,])
      Lambda=lambda*(S-S_prec)^2
      Lambda[1,]=0
      loss=sum(S^2*V+Lambda)
      
      # M step
      for(k in 1:K){
        #mu[k,]=apply(Ycont, 2, function(x) weighted_median(x, weights = S[,k]))
        #mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k])})
        mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k]^2)})
        if(cat_flag){
          if(n_cat==1){
            #mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k])
            mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k]^2)
            
          }
          else{
            #mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k])})
            mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k]^2)})
            
          }
          #mo[k,]=apply(Ycat,2,function(x)weighted_mode(x,weights=S[,k]))
        }
      }
      
      if(cat_flag){
        mumo[,cat.indx]=mo
      }
      
      mumo[,cont.indx]=mu
      colnames(mumo)=colnames(Y)
      
      
      if (verbose) {
        cat(sprintf('Iteration %d: %.6e\n', it, loss))
      }
      if (!is.null(tol)) {
        epsilon <- loss_old - loss
        if (epsilon < tol) {
          break
        }
      } else if (all(S == S_old)) {
        break
      }
      loss_old <- loss
      S_old=S
    }
    if (is.null(best_S) || (loss_old < best_loss)) {
      best_loss <- loss_old
      best_S <- S
    }
    s=initialize_states(Y,K)
  }
  
  old_MAP=apply(best_S,1,which.max)
  MAP=order_states_condMed(Y[,cont.indx[1]],old_MAP)
  
  tab <- table(MAP, old_MAP)
  new_order <- apply(tab, 1, which.max)
  
  # Reorder the columns of S accordingly
  best_S <- best_S[, new_order]
  
  
  # res_Y=data.frame(Y,MAP=MAP)
  # col_sort=as.integer(names(sort(tapply(res_Y[,cont.indx[1]],
  #                                       res_Y$MAP,mean))))
  #mumo=mumo[col_sort,]
  # best_S=best_S[,col_sort]
  
  return(list(best_S=best_S,
              MAP=MAP,
              Y=Y
              # ,
              # condMM=mumo
              ))
}

######
fuzzy_jump_coord <- function(Y, 
                         K, 
                         lambda=1e-5, 
                         m=1,
                         max_iter=5, 
                         n_init=10, tol=1e-16, 
                         verbose=FALSE
                         
) {
  # Fit jump model for mixed type data 
  
  # Arguments:
  # Y: data.frame with mixed data types. Categorical variables must be factors.
  # K: number of states
  # lambda: penalty for the number of jumps
  # initial_states: initial state sequence
  # max_iter: maximum number of iterations
  # n_init: number of initializations
  # tol: tolerance for convergence
  # verbose: print progress
  # time_vec is a vector of time points, needed if times are not equally sampled
  
  # Value:
  # best_s: estimated state sequence
  # Y: imputed data
  # Y.orig: original data
  # condMM: state-conditional medians and modes
  
  K=as.integer(K)
  
  TT <- nrow(Y)
  P <- ncol(Y)
  best_loss <- NULL
  best_S <- NULL
  
  # Which vars are categorical and which are numeric
  cat_flag=any(sapply(Y, is.factor))
  
  if(cat_flag){
    cat.indx=which(sapply(Y, is.factor))
    cont.indx=which(sapply(Y, is.numeric))
    Ycont=Y[,cont.indx]
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    Ycat=Y[,cat.indx]
    
    if(length(cat.indx)==1){
      n_levs=length(levels(Ycat))
    }
    else{
      n_levs=apply(Ycat, 2, function(x)length(unique(x[!is.na(x)])))
    }
    
    n_cat=length(cat.indx)
    n_cont=P-n_cat
    
  }
  else{
    cont.indx=1:P
    Ycont=Y
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    n_cont=dim(Y)[2]
    n_cat=0
  }
  
  
  
  
  for (init in 1:n_init) {
    
    # k-prot++
    s=initialize_states(Y,K)
    S <- matrix(0, nrow = TT, ncol = K)
    row_indices <- rep(1:TT)  
    S[cbind(row_indices, s)] <- 1 
    
    
    # initialize mumo
    mu <- matrix(NA, nrow=K, ncol=length(cont.indx))
    if(cat_flag){
      mo <- matrix(NA, nrow=K, ncol=length(cat.indx))
    }
    
    for(k in 1:K){
      mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k]^m)})
      if(cat_flag){
        if(n_cat==1){
          mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k]^m)
          
        }
        else{
          mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k]^m)})
        }
      }
    }
    
    mumo=data.frame(matrix(0,nrow=K,ncol=P))
    if(cat_flag){
      mumo[,cat.indx]=mo
    }
    
    mumo[,cont.indx]=mu
    colnames(mumo)=colnames(Y)
    
    if(cat_flag){
      for(p in 1:n_cat){
        if(n_cat==1){
          mumo[,cat.indx[p]]=factor(mumo[,cat.indx[p]],levels=levels(Ycat[p]))
        }
        else{
          mumo[,cat.indx[p]]=factor(mumo[,cat.indx[p]],levels=levels(Ycat[,p]))
        }
      }
    }
    
    S_old=S
    V=gower.dist(Y,mumo)
    loss_old=sum(V*S^m)+lambda*sum(abs(S[1:(TT-1),]-S[2:TT,]))^2
    
    for (it in 1:max_iter) {
      
      # S(1)
      
      result <- Rsolnp::solnp(#pars = S[1,],
        pars = rep(1/K,K),
                      fun = function(s) objective_function_1T(s, 
                                                              g_values=V[1,], 
                                                              lambda=lambda, 
                                                              s_t_1=S[2,], 
                                                              m=m),
                      eqfun = function(s) sum(s),
                      eqB = 1,
                      LB = rep(0, K),
                      control = list(trace = 0))
      
      S[1,] <- result$pars
      
      # S(2) to S(T-1)
      
      for(t in 2:(TT-1)){
        result=Rsolnp::solnp(
          #pars = S[t,],
          pars = rep(1/K,K),
                             fun = function(s) objective_function(s, 
                                                                  g_values=V[t,], 
                                                                  lambda=lambda, 
                                                                  s_t_prec=S[t-1,],
                                                                  s_t_succ=S[t+1,],
                                                                  m=m),
                             eqfun = function(s) sum(s),
                             eqB = 1,
                             LB = rep(0, K),
                             control = list(trace = 0))
        S[t,]=result$pars
        
      }
      
      # S(T)
      result <- Rsolnp::solnp(
        #pars = S[TT,],
        pars = rep(1/K,K),
                              fun = function(s) objective_function_1T(s, 
                                                                      g_values=V[TT,], 
                                                                      lambda=lambda, 
                                                                      s_t_1=S[TT-1,], 
                                                                      m=m),
                              eqfun = function(s) sum(s),
                              eqB = 1,
                              LB = rep(0, K),
                              control = list(trace = 0))
      
      S[TT,] <- result$pars
      

      for(k in 1:K){
        #mu[k,]=apply(Ycont, 2, function(x) weighted_median(x, weights = S[,k]))
        #mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k])})
        mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=S[,k]^m)})
        if(cat_flag){
          if(n_cat==1){
            #mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k])
            mo[k,]=poliscidata::wtd.mode(Ycat,weights=S[,k]^m)
            
          }
          else{
            #mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k])})
            mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=S[,k]^m)})
            
          }
          #mo[k,]=apply(Ycat,2,function(x)weighted_mode(x,weights=S[,k]))
        }
      }
      
      if(cat_flag){
        mumo[,cat.indx]=mo
      }
      
      mumo[,cont.indx]=mu
      colnames(mumo)=colnames(Y)
      if(cat_flag){
        for(p in 1:n_cat){
          if(n_cat==1){
            mumo[,cat.indx[p]]=factor(mumo[,cat.indx[p]],levels=levels(Ycat[p]))
          }
          else{
          mumo[,cat.indx[p]]=factor(mumo[,cat.indx[p]],levels=levels(Ycat[,p]))
          }
        }
      }
      
      V=gower.dist(Y,mumo)
      loss=sum(V*S^m)+lambda*sum(abs(S[1:(TT-1),]-S[2:TT,]))^2
      
      if (verbose) {
        cat(sprintf('Iteration %d: %.6e\n', it, loss))
      }
      if (!is.null(tol)) {
        epsilon <- loss_old - loss
        if (epsilon < tol) {
          break
        }
      } else if (all(S == S_old)) {
        break
      }
      loss_old <- loss
      S_old=S
    }
    if (is.null(best_S) || (loss_old < best_loss)) {
      best_loss <- loss_old
      best_S <- S
    }
  }
  
  old_MAP=apply(best_S,1,which.max)
  MAP=order_states_condMed(Y[,cont.indx[1]],old_MAP)
  
  tab <- table(MAP, old_MAP)
  new_order <- apply(tab, 1, which.max)
  
  # Reorder the columns of S accordingly
  best_S <- best_S[, new_order]

  # res_Y=data.frame(Y,MAP=MAP)
  # col_sort=as.integer(names(sort(tapply(res_Y[,cont.indx[1]],
  #                                       res_Y$MAP,mean))))
  #mumo=mumo[col_sort,]
  # best_S=best_S[,col_sort]
  
  return(list(best_S=best_S,
              MAP=MAP,
              Y=Y
              # ,
              # condMM=mumo
  ))
}

fuzzy_jump_coord_par <- function(Y, 
                             K, 
                             lambda=1e-5, 
                             m=1.01,
                             max_iter=5, 
                             n_init=10, tol=1e-16, 
                             verbose=FALSE,
                             n_cores=NULL
                             
) {
  # Fit jump model for mixed type data 
  
  # Arguments:
  # Y: data.frame with mixed data types. Categorical variables must be factors.
  # K: number of states
  # lambda: penalty for the number of jumps
  # initial_states: initial state sequence
  # max_iter: maximum number of iterations
  # n_init: number of initializations
  # tol: tolerance for convergence
  # verbose: print progress
  # time_vec is a vector of time points, needed if times are not equally sampled
  
  # Value:
  # best_s: estimated state sequence
  # Y: imputed data
  # Y.orig: original data
  # condMM: state-conditional medians and modes
  
  K=as.integer(K)
  
  TT <- nrow(Y)
  P <- ncol(Y)
  best_loss <- NULL
  best_S <- NULL
  
  # Which vars are categorical and which are numeric
  cat_flag=any(sapply(Y, is.factor))
  
  if(cat_flag){
    cat.indx=which(sapply(Y, is.factor))
    cont.indx=which(sapply(Y, is.numeric))
    Ycont=Y[,cont.indx]
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    Ycat=Y[,cat.indx]
    
    if(length(cat.indx)==1){
      n_levs=length(levels(Ycat))
    }
    else{
      n_levs=apply(Ycat, 2, function(x)length(unique(x[!is.na(x)])))
    }
    
    n_cat=length(cat.indx)
    n_cont=P-n_cat
    
  }
  else{
    cont.indx=1:P
    Ycont=Y
    Ycont=apply(Ycont,2,scale)
    Y[,cont.indx]=Ycont
    n_cont=dim(Y)[2]
    n_cat=0
  }
  
  
  # Imposta i core da usare
  if(is.null(n_cores)){
    n_cores <- parallel::detectCores() - 1
  }
  cl <- makeCluster(n_cores)
  registerDoParallel(cl)
  
  # Esegui le inizializzazioni in parallelo
  results <- foreach(init = 1:n_init, .packages = c("poliscidata", "Rsolnp",
                                                    "cluster","StatMatch"),
                     .export = c("initialize_states",
                                 "objective_function_1T",
                                 "objective_function")) %dopar% {
    
    s = initialize_states(Y, K)
    S <- matrix(0, nrow = TT, ncol = K)
    row_indices <- rep(1:TT)
    S[cbind(row_indices, s)] <- 1
    
    mu <- matrix(NA, nrow=K, ncol=length(cont.indx))
    if(cat_flag){
      mo <- matrix(NA, nrow=K, ncol=length(cat.indx))
    }
    
    for(k in 1:K){
      mu[k,] = apply(Ycont, 2, function(x) poliscidata::wtd.median(x, weights = S[,k]^m))
      if(cat_flag){
        if(n_cat == 1){
          mo[k,] = poliscidata::wtd.mode(Ycat, weights = S[,k]^m)
        } else {
          mo[k,] = apply(Ycat, 2, function(x) poliscidata::wtd.mode(x, weights = S[,k]^m))
        }
      }
    }
    
    mumo <- data.frame(matrix(0, nrow=K, ncol=P))
    if(cat_flag){
      mumo[,cat.indx] <- mo
    }
    mumo[,cont.indx] <- mu
    colnames(mumo) <- colnames(Y)
    if(cat_flag){
      for(p in 1:n_cat){
        if(n_cat==1){
          mumo[,cat.indx[p]] <- factor(mumo[,cat.indx[p]], levels=levels(Ycat[p]))
        }
        else{
          mumo[,cat.indx[p]] <- factor(mumo[,cat.indx[p]], levels=levels(Ycat[,p]))
        }
      }
    }
    
    S_old <- S
    V <- gower.dist(Y, mumo)
    loss_old <- sum(V * S^m) + lambda * sum(abs(S[1:(TT-1), ] - S[2:TT, ]))^2
    
    for (it in 1:max_iter) {
      result <- Rsolnp::solnp(
        pars = rep(1/K, K),
        fun = function(s) objective_function_1T(s, g_values=V[1,], lambda=lambda, s_t_1=S[2,], m=m),
        eqfun = function(s) sum(s),
        eqB = 1,
        LB = rep(0, K),
        control = list(trace = 0)
      )
      S[1,] <- result$pars
      
      for(t in 2:(TT-1)){
        result <- Rsolnp::solnp(
          pars = rep(1/K, K),
          fun = function(s) objective_function(s, g_values=V[t,], lambda=lambda, s_t_prec=S[t-1,], s_t_succ=S[t+1,], m=m),
          eqfun = function(s) sum(s),
          eqB = 1,
          LB = rep(0, K),
          control = list(trace = 0)
        )
        S[t,] <- result$pars
      }
      
      result <- Rsolnp::solnp(
        pars = rep(1/K, K),
        fun = function(s) objective_function_1T(s, g_values=V[TT,], lambda=lambda, s_t_1=S[TT-1,], m=m),
        eqfun = function(s) sum(s),
        eqB = 1,
        LB = rep(0, K),
        control = list(trace = 0)
      )
      S[TT,] <- result$pars
      
      
      for(k in 1:K){
        mu[k,] = apply(Ycont, 2, function(x) poliscidata::wtd.median(x, weights = S[,k]^m))
        if(cat_flag){
          if(n_cat == 1){
            mo[k,] = poliscidata::wtd.mode(Ycat, weights = S[,k]^m)
          } else {
            mo[k,] = apply(Ycat, 2, function(x) poliscidata::wtd.mode(x, weights = S[,k]^m))
          }
        }
      }
      
      if(cat_flag){
        mumo[,cat.indx] <- mo
      }
      mumo[,cont.indx] <- mu
      colnames(mumo) <- colnames(Y)
      if(cat_flag){
        for(p in 1:n_cat){
          if(n_cat==1){
            mumo[,cat.indx[p]] <- factor(mumo[,cat.indx[p]], levels=levels(Ycat[p]))
          }
          else{
            mumo[,cat.indx[p]] <- factor(mumo[,cat.indx[p]], levels=levels(Ycat[,p]))
          }
        }
      }
      
      V <- gower.dist(Y, mumo)
      
      loss <- sum(V * S^m) + lambda * sum(abs(S[1:(TT-1), ] - S[2:TT, ]))^2
      
      if(!is.null(tol)){
        if((loss_old - loss) < tol) break
      } else if(all(S == S_old)) {
        break
      }
      loss_old <- loss
      S_old <- S
    }
    
    list(S = S, loss = loss_old)
  }
  
  # Trova il migliore
  losses <- sapply(results, function(res) res$loss)
  best_index <- which.min(losses)
  best_S <- results[[best_index]]$S
  best_loss <- losses[best_index]
  
  # Ferma il cluster
  stopCluster(cl)
  
  
  
  old_MAP=apply(best_S,1,which.max)
  MAP=order_states_condMed(Y[,cont.indx[1]],old_MAP)
  
  tab <- table(MAP, old_MAP)
  new_order <- apply(tab, 1, which.max)
  
  # Reorder the columns of S accordingly
  best_S <- best_S[, new_order]
  
  return(list(best_S=best_S,
              MAP=MAP,
              Y=Y
  ))
}

simstud_fuzzyJM_coord=function(seed,
                               lambda,
                               TT,
                               P,
                               K,
                               mu=1.5,
                               phi=.8,
                               rho=0,
                               Pcat=NULL,
                               pers=.9,
                               m=1.5,
                               nu=4){
  # Simulate
  simDat=sim_data_stud_t(seed=123,
                         TT=TT,
                         P=P,
                         Pcat=Pcat,
                         Ktrue=K,
                         mu=mu,
                         rho=rho,
                         nu=nu,
                         phi=phi,
                         pers=pers)
  
  Y=simDat$SimData
  
  success <- FALSE
  trials=1
  while (!success&trials<10) {
    est <- try(fuzzy_jump_coord(Y=Y, 
                                K=K, 
                                lambda=lambda, 
                                m=m,
                                max_iter=10, 
                                n_init=10, tol=1e-8, 
                                verbose=FALSE
                                
    ), silent = TRUE)
    trials=trials+1
    
    if (!inherits(est, "try-error")) {
      success <- TRUE  # Exit the loop if no error
    } else {
      message("Retrying fuzzy_jump() due to an error...")
    }
  }
  
  
  
  old_MAP=factor(est$MAP,levels=1:K)
  MAP=factor(relabel_clusters(est$MAP,simDat$mchain),levels=1:K)
  #est$MAP=factor(relabel_clusters(est$MAP,simDat$mchain),levels=1:K)
  simDat$mchain=factor(simDat$mchain,levels=1:K)
  
  BAC=caret::confusionMatrix(MAP,simDat$mchain)$overall[1]
  ARI=adj.rand.index(MAP,simDat$mchain)
  
  tab <- table(MAP, old_MAP)
  new_order <- apply(tab, 1, which.max)
  best_S=est$best_S[, new_order]
  
  fRI=fclust::ARI.F(simDat$mchain,best_S)
  
  # Return
  return(list(
    S=best_S,
    MAP=MAP ,
    ARI=ARI,
    BAC=BAC,
    fRI=fRI,
    seed=seed,
    lambda=lambda,
    TT=TT,
    P=P,
    K=K,
    Pcat=Pcat,
    true_seq=simDat$mchain
  ))
  
}

######

relabel_clusters <- function(predicted, true) {
  mapping <- apply(table(predicted, true), 1, which.max)
  sapply(predicted, function(x) mapping[x])
  #return(as.factor(predicted))
}


simstud_fuzzyJM=function(seed,lambda,TT,P,
                         K=3,mu=1,
                         phi=.8,rho=0,
                         Pcat=NULL,pers=.95,
                         pNAs=0,typeNA=2,m=1.01){
  # Simulate
  simDat=sim_data_mixed(seed=seed,
                        TT=TT,
                        P=P,
                        Ktrue=K,
                        mu=mu,
                        phi=phi,
                        rho=rho,
                        Pcat=Pcat,
                        pers=pers)
  
  Y=simDat$SimData.complete
  # Estimate
  # est=fuzzy_jump(Y, 
  #                n_states=K, jump_penalty=lambda, 
  #                initial_states=NULL,
  #                max_iter=10, n_init=10, tol=NULL, 
  #                verbose=FALSE)
  success <- FALSE
  trials=1
  while (!success&trials<10) {
    est <- try(fuzzy_jump(Y, 
                          n_states = K, jump_penalty = lambda, 
                          initial_states = NULL,
                          max_iter = 10, n_init = 10, tol = NULL, 
                          verbose = FALSE), silent = TRUE)
    trials=trials+1
    
    if (!inherits(est, "try-error")) {
      success <- TRUE  # Exit the loop if no error
    } else {
      message("Retrying fuzzy_jump() due to an error...")
    }
  }
  
  est$MAP=factor(relabel_clusters(est$MAP,simDat$mchain),levels=1:K)
  simDat$mchain=factor(simDat$mchain,levels=1:K)
  
  BAC=caret::confusionMatrix(est$MAP,simDat$mchain)$overall[1]
  
  ARI=adj.rand.index(est$MAP,simDat$mchain)
  
  # Return
  return(list(
    S=est$best_S,
    MAP=est$MAP ,
    ARI=ARI,
    BAC=BAC,
    #,
    seed=seed,
    lambda=lambda,
    TT=TT,
    P=P,
    # Ktrue=K,
    # mu=mu,
    # phi=phi,
    # rho=rho,
    # Pcat=Pcat,
    # pers=pers,
    true_seq=simDat$mchain
    # est_seq=MAP
  ))
  
}

simstud_fuzzyJM_m=function(seed,lambda,TT,P,
                         K=3,mu=1,
                         phi=.8,rho=0,
                         Pcat=NULL,pers=.95,
                         pNAs=0,typeNA=2,m){
  # Simulate
  simDat=sim_data_mixed(seed=seed,
                        TT=TT,
                        P=P,
                        Ktrue=K,
                        mu=mu,
                        phi=phi,
                        rho=rho,
                        Pcat=Pcat,
                        pers=pers)
  
  Y=simDat$SimData.complete
  
  # success <- FALSE
  # trials=1
  # while (!success&trials<10) {
    est <- 
      #try(
      fuzzy_jump_m(Y=Y, 
                            K = K, lambda = lambda, 
                            m=m,
                            initial_states = NULL,
                            max_iter = 10, n_init = 10, tol = NULL, 
                            verbose = FALSE)
      #, silent = TRUE)
    #trials=trials+1
    
    # if (!inherits(est, "try-error")) {
    #   success <- TRUE  # Exit the loop if no error
    # } else {
    #   message("Retrying fuzzy_jump() due to an error...")
    # }
  #}
  
  #matplot(est$best_S,type='l',main=paste0("m = ", m))
  
  true_states=simDat$mchain
  if(is.null(simDat$Pcat)){
    true_states=order_states_condMed(Y[,1],true_states)
  }
  else{
    true_states=order_states_condMed(Y[,simDat$Pcat+1],true_states)
  }
  
  MAP=factor(est$MAP,levels=1:K)
  true_states=factor(true_states,levels=1:K)
  
  BAC=as.numeric(
    caret::confusionMatrix(MAP,true_states)$overall[1]
  )
  ARI=adj.rand.index(MAP,true_states)
  
  S=matrix(unlist(est[1]),ncol=K,nrow=TT,byrow=F)
  
  # Return
  return(list(
    S=S,
    MAP=MAP ,
    ARI=ARI,
    BAC=BAC,
    seed=seed,
    lambda=lambda,
    TT=TT,
    P=P,
    m=m,
    true_seq=true_states
  ))
  
}


# Cont JM -----------------------------------------------------------------

discretize_prob_simplex <- function(n_c, grid_size) {
  # Sample grid points on a probability simplex.
  N <- as.integer(1 / grid_size)
  
  # Generate all combinations and filter those that sum to N
  tuples <- expand.grid(rep(list(0:N), n_c))
  valid_tuples <- tuples[rowSums(tuples) == N, ]
  
  # Reverse the order and scale to get the simplex points
  lst <- as.matrix(valid_tuples[nrow(valid_tuples):1, ]) / N
  rownames(lst) <- NULL
  return(lst)
}

onerun_contJM=function(Y,K,
                       jump_penalty,alpha,grid_size,mode_loss=T,
                       max_iter,tol,initial_states,M){
  tryCatch({
    TT=nrow(Y)
    loss_old <- 1e10
    
    S_old=matrix(0,nrow=TT,ncol=K)
    
    # State initialization through kmeans++
    if (!is.null(initial_states)) {
      s <- initial_states
    } else {
      #s <- initialize_states_jumpR(Y, K)
      s=maotai::kmeanspp(Y,k=K)
    }
    mu=matrix(NA,nrow=K,ncol=P)
    for (i in unique(s)) {
      
      if(sum(s==i)>1){
        mu[i,] <- colMeans(Y[s==i,])
      }
      else{
        mu[i,]=mean(Y[s==i,])
      }
    }
    
    for (it in 1:max_iter) {
      
      # E Step
      
      # Compute loss matrix
      loss_mx <- matrix(NA, nrow(Y), nrow(mu))
      for (t in 1:nrow(Y)) {
        for (k in 1:nrow(mu)) {
          loss_mx[t, k] <- .5*sqrt(sum((Y[t, ] - mu[k, ])^2))
        }
      }
      
      # Continuous model
      prob_vecs <- discretize_prob_simplex(K, grid_size)
      pairwise_l1_dist <- as.matrix(dist(prob_vecs, method = "manhattan")) / 2
      jump_penalty_mx <- jump_penalty * (pairwise_l1_dist ^ alpha)
      
      if (mode_loss) {
        # Adding mode loss
        m_loss <- log(rowSums(exp(-jump_penalty_mx)))
        m_loss <- m_loss - m_loss[1]  # Offset a constant
        jump_penalty_mx <- jump_penalty_mx + m_loss
      }
      
      LARGE_FLOAT=1e1000
      # Handle continuous model with probability vectors
      if (!is.null(prob_vecs)) {
        loss_mx[is.nan(loss_mx)] <- LARGE_FLOAT
        loss_mx <- loss_mx %*% t(prob_vecs)
      }
      
      
      N <- ncol(loss_mx)
      
      loss_mx[is.nan(loss_mx)] <- Inf
      
      # DP algorithm initialization
      values <- matrix(NA, TT, N)
      assign <- integer(TT)
      
      # Initial step
      values[1, ] <- loss_mx[1, ]
      
      # DP iteration (bottleneck)
      for (t in 2:TT) {
        values[t, ] <- loss_mx[t, ] + apply(values[t - 1, ] + jump_penalty_mx, 2, min)
      }
      
      S=matrix(NA,nrow=TT,ncol=K)
      
      # Find optimal path backwards
      assign[TT] <- which.min(values[TT, ])
      value_opt <- values[TT, assign[TT]]
      
      S[TT,]=prob_vecs[assign[TT],]
      
      # Traceback
      for (t in (TT - 1):1) {
        assign[t] <- which.min(values[t, ] + jump_penalty_mx[, assign[t + 1]])
        S[t,]=prob_vecs[assign[t],]
      }
      
      # M Step
      
      for(k in 1:K){
        # What if the denominator is 0??
        mu[k,]=apply(Y, 2, function(x) weighted.mean(x, w = S[,k]))
      }
      
      # Re-fill-in missings
      for(i in 1:P){
        Y[,i]=ifelse(M[,i],mu[apply(S,1,which.max),i],Y[,i])
      }
      
      if (!is.null(tol)) {
        epsilon <- loss_old - value_opt
        if (epsilon < tol) {
          break
        }
      } 
      
      else if (all(S == S_old)) {
        break
      }
      
      S_old=S
      
      loss_old <- value_opt
    }
    
    
    
    return(list(S=S,value_opt=value_opt))}, error = function(e) {
      # Return a consistent placeholder on error
      return(list(S = NA, value_opt = Inf))
    })
}

cont_jumpR <- function(Y, 
                       K, 
                       jump_penalty=1e-5, 
                       alpha=2,
                       initial_states=NULL,
                       max_iter=10, 
                       n_init=10, 
                       tol=NULL, 
                       mode_loss=T,
                       #method="euclidean",
                       grid_size=.03,
                       prll=F,
                       n_cores=NULL
) {
  # Fit jump model using framework of Bemporad et al. (2018)
  
  Y=as.matrix(Y)
  K=as.integer(K)
  
  TT <- nrow(Y)
  P <- ncol(Y)
  
  # Initialize mu
  mu <- colMeans(Y,na.rm = T)
  
  # Track missings with 0 1 matrix
  M=ifelse(is.na(Y),T,F)
  
  
  Ytil=Y
  # Impute missing values with mean of observed states
  for(i in 1:P){
    Y[,i]=ifelse(M[,i],mu[i],Y[,i])
  }
  
  
  if(prll){
    if(is.null(n_cores)){
      n_cores=parallel::detectCores()-1
    }
    hp_init=expand.grid(init=1:n_init)
    jms <- parallel::mclapply(1:nrow(hp_init),
                              function(x)
                                onerun_contJM(Y=Y,K=K,
                                              jump_penalty=jump_penalty,
                                              alpha=alpha,grid_size=grid_size,
                                              mode_loss=mode_loss,
                                              max_iter=max_iter,tol=tol,
                                              initial_states=initial_states,
                                              M=M),
                              mc.cores = n_cores)
  }
  else{
    jms=list()
    for (init in 1:n_init) {
      
      jms[[init]]=onerun_contJM(Y,K,
                                jump_penalty,alpha,grid_size,mode_loss,
                                max_iter,tol,initial_states,M=M)
      
      # #mu <- matrix(0, nrow=K, ncol=P)
      # loss_old <- 1e10
      # 
      # S_old=matrix(0,nrow=TT,ncol=K)
      # 
      # # State initialization through kmeans++
      # if (!is.null(initial_states)) {
      #   s <- initial_states
      # } else {
      #   #s <- initialize_states_jumpR(Y, K)
      #   s=maotai::kmeanspp(Y,k=K)
      # }
      # mu=matrix(NA,nrow=K,ncol=P)
      # for (i in unique(s)) {
      #   
      #   if(sum(s==i)>1){
      #     mu[i,] <- colMeans(Y[s==i,])
      #   }
      #   else{
      #     mu[i,]=mean(Y[s==i,])
      #   }
      # }
      # 
      # for (it in 1:max_iter) {
      #   
      #   # E Step
      #   
      #   # Compute loss matrix
      #   loss_mx <- matrix(NA, nrow(Y), nrow(mu))
      #   for (t in 1:nrow(Y)) {
      #     for (k in 1:nrow(mu)) {
      #       loss_mx[t, k] <- .5*sqrt(sum((Y[t, ] - mu[k, ])^2))
      #     }
      #   }
      #   
      #   # Continuous model
      #   prob_vecs <- discretize_prob_simplex(K, grid_size)
      #   pairwise_l1_dist <- as.matrix(dist(prob_vecs, method = "manhattan")) / 2
      #   jump_penalty_mx <- jump_penalty * (pairwise_l1_dist ^ alpha)
      #   
      #   if (mode_loss) {
      #     # Adding mode loss
      #     m_loss <- log(rowSums(exp(-jump_penalty_mx)))
      #     m_loss <- m_loss - m_loss[1]  # Offset a constant
      #     jump_penalty_mx <- jump_penalty_mx + m_loss
      #   }
      #   
      #   LARGE_FLOAT=1e1000
      #   # Handle continuous model with probability vectors
      #   if (!is.null(prob_vecs)) {
      #     loss_mx[is.nan(loss_mx)] <- LARGE_FLOAT
      #     loss_mx <- loss_mx %*% t(prob_vecs)
      #   }
      #   
      #   
      #   N <- ncol(loss_mx)
      #   
      #   loss_mx[is.nan(loss_mx)] <- Inf
      #   
      #   # DP algorithm initialization
      #   values <- matrix(NA, TT, N)
      #   assign <- integer(TT)
      #   
      #   # Initial step
      #   values[1, ] <- loss_mx[1, ]
      # 
      #   # DP iteration (bottleneck)
      #   for (t in 2:TT) {
      #     values[t, ] <- loss_mx[t, ] + apply(values[t - 1, ] + jump_penalty_mx, 2, min)
      #   }
      #   
      #   S=matrix(NA,nrow=TT,ncol=K)
      #   
      #   # Find optimal path backwards
      #   assign[TT] <- which.min(values[TT, ])
      #   value_opt <- values[TT, assign[TT]]
      #   
      #   S[TT,]=prob_vecs[assign[TT],]
      #   
      #   # Traceback
      #   for (t in (TT - 1):1) {
      #     assign[t] <- which.min(values[t, ] + jump_penalty_mx[, assign[t + 1]])
      #     S[t,]=prob_vecs[assign[t],]
      #   }
      #   
      #   # M Step
      #   
      #   for(k in 1:K){
      #     # What if the denominator is 0??
      #     mu[k,]=apply(Y, 2, function(x) weighted.mean(x, w = S[,k]))
      #   }
      #   
      #   # Re-fill-in missings
      #   for(i in 1:P){
      #     Y[,i]=ifelse(M[,i],mu[apply(S,1,which.max),i],Y[,i])
      #   }
      # 
      #   
      #   # if (length(unique(S)) == 1) {
      #   #   break
      #   # }
      #   
      #   #loss <- min(V[1,])
      #   if (verbose) {
      #     cat(sprintf('Iteration %d: %.6e\n', it, value_opt))
      #   }
      #   if (!is.null(tol)) {
      #     epsilon <- loss_old - value_opt
      #     if (epsilon < tol) {
      #       break
      #     }
      #   } 
      #   else if (all(S == S_old)) {
      #     break
      #   }
      #   
      #   S_old=S
      #   
      #   loss_old <- value_opt
      # }
      
      # if (is.null(best_S) || (loss_old < best_loss)) {
      #   best_loss <- loss_old
      #   best_S <- S
      # }
      #s <- initialize_states_jumpR(Y, K)
      
    }
  }
  
  best_init=which.min(unlist(lapply(jms,function(x)x$value_opt)))
  best_S=jms[[best_init]]$S
  
  # old_lab=apply(best_S,1,which.max)
  # 
  # new_lab=order_states_condMean(
  #   Y[,1],old_lab
  #   )
  # 
  # mapping <- tapply(old_lab, new_lab, function(x) unique(x))
  # mapping <- as.integer(unlist(mapping))
  # best_S <- best_S[, mapping]
  
  return(best_S)
}


# Spatio-temporal ---------------------------------------------------------

simulate_observations <- function(mu=1,rho=0,phi=.8,n_states=3,P,Pcat,M,s,seed
                                  #,pNAs,typeNA=0
) {
  
  # This function simulates data from a multivariate normal distribution given the latent states sequence
  
  # Arguments:
  # mu: Mean values for data simulation (first state has mean = mu, last state has mean = -mu, and all intermediate states are equally spaced between them)
  # rho: Correlation between variables
  # n_states: Number of states
  # P: Number of features
  # Pcat: Number of categorical features
  # M: Number of spatial points
  # s: Latent states sequence
  # seed: Random seed
  # pNAs: Percentage of missing values
  # tpNA: Type of missing values (0 = random missing pattern, 1 = block (continuous) missing pattern)
  
  
  if(is.null(Pcat)){
    Pcat=floor(P/2)
  }
  
  MU=mu
  mu=seq(-mu,mu,length.out=n_states)
  
  Sigma <- matrix(rho,ncol=P,nrow=P)
  diag(Sigma)=1
  
  Sim = matrix(0, M, P * n_states)
  SimData = matrix(0, M, P)
  
  set.seed(seed)
  for(k in 1:n_states){
    u = MASS::mvrnorm(M,rep(mu[k],P),Sigma)
    Sim[, (P * k - P + 1):(k * P)] = u
  }
  
  for (i in 1:M) {
    k = s[i]
    SimData[i, ] = Sim[i, (P * k - P + 1):(P * k)]
  }
  
  if(Pcat!=0){
    SimData[,1:Pcat]=apply(SimData[,1:Pcat],2,get_cat,mc=s,mu=MU,phi=phi)
    SimData=as.data.frame(SimData)
    SimData[,1:Pcat]=SimData[,1:Pcat]%>%mutate_all(as.factor)
  }
  
  # if(pNAs>0){
  #   SimData.NA=apply(SimData,2,punct,pNAs=pNAs,type=typeNA)
  #   SimData.NA=as.data.frame(SimData.NA)
  #   SimData.NA[,1:Pcat]=SimData.NA[,1:Pcat]%>%mutate_all(as.factor)
  #   SimData.NA[,-(1:Pcat)]=SimData.NA[,-(1:Pcat)]%>%mutate_all(as.numeric)
  # }
  
  #else{
  SimData.NA=SimData
  #}
  
  return(
    #list(
    SimData=SimData
    #,SimData.NA=SimData.NA
    #)
  )
  
}

# Function to create spatial points
generate_spatial_points <- function(n, max_distance = 10) {
  x <- runif(n, 0, max_distance)
  y <- runif(n, 0, max_distance)
  return(data.frame(x = x, y = y))
}

generate_spatio_temporal_data <- function(M, TT, theta, beta, K = 3,
                                          mu=1,rho=0,phi=.8,
                                          P,Pcat,seed,
                                          pGap=.2,
                                          pNAs=0) {
  
  
  # Function to generate spatio-temporal data with spatial and temporal persistence
  
  # Arguments:
  # M: Number of spatial points
  # TT: Number of time points
  # theta: Spatial correlation parameter (the lower, the higher the spatial correlation)
  # beta: Temporal correlation parameter (the higher, the higher the temporal correlation)
  # K: Number of states (only K=3 available at the moment)
  # mu: Mean values for data simulation (first state has mean = mu, last state has mean = -mu, and all intermediate states are equally spaced between them)
  # rho: Correlation between variables
  # phi: Conditional probability for the categorical outcome k in state k
  # P: Number of features
  # Pcat: Number of categorical features
  # seed: Random seed
  # pGap: Percentage of time points to be removed 
  # pNAs: Percentage of missing values (only random missing pattern is available at the moment)
  
  # Value:
  # A list with the following elements:
  # S: A matrix TTxM with the simulated states
  # Y: A data frame with the complete simulated data in long format
  # Y.NA: A data frame with the simulated data with missing values in long format
  # spatial_points: A data frame with the spatial points
  # spatial_cov: The spatial covariance matrix
  # dist_matrix: The distance matrix
  
  
  
  # Increment TT by one as the first time step will be removed
  TT=TT+1
  
  
  # Generate spatial points
  spatial_points <- generate_spatial_points(M)
  
  # Create spatial covariance matrix based on distance and theta
  dist_matrix <- as.matrix(dist(spatial_points)) # Eventually substitute with Gower distance
  
  spatial_cov <- exp(-theta * dist_matrix)
  
  # Initialize data matrix
  data <- array(0, dim = c(TT, M))
  
  S=matrix(0,TT,M)
  
  # Initial time step data (from spatial process)
  data[1, ] <- mvrnorm(1, mu = rep(0, M), Sigma = spatial_cov)
  
  cluster_levels <- quantile(data[1,], probs = seq(0, 1, length.out = K + 1))
  S[1,] <- cut(data[1,], breaks = cluster_levels, labels = FALSE,
               include.lowest =T)
  
  temp=simulate_observations(mu=mu,rho=rho,phi=phi,
                             n_states=K,P=P,Pcat=Pcat,M=M,
                             s=S[1,],seed=seed+seed*1000
                             #,pNAs=pNAs,typeNA=0
  )
  
  Y=temp#$SimData
  #Y.NA=temp$SimData.NA
  
  Y=data.frame(Y)
  Y$m=1:M
  Y$t=rep(0,M)
  
  S[1,]=order_states_condMean(Y[Y$t==0,dim(Y)[2]-2],S[1,])
  
  # Y.NA=data.frame(Y.NA)
  # Y.NA$m=1:M
  # Y.NA$t=rep(0,M)
  
  # Generate data for each subsequent time step
  for (t in 2:TT) {
    eta_t <- mvrnorm(1, mu = rep(0, M), Sigma = spatial_cov)  # Spatial noise
    data[t, ] <- beta * data[t-1, ] + eta_t  # Temporal correlation
    
    #data[t, ] <- beta * data[t-1, ] + (1-beta)*eta_t  # Temporal correlation
    
    cluster_levels <- quantile(data[t,], probs = seq(0, 1, length.out = K + 1))
    S[t,] <- cut(data[t,], breaks = cluster_levels, labels = FALSE,
                 include.lowest =T)
    
    simDat=simulate_observations(mu=mu,rho=rho,phi=phi,
                                 n_states=K,P=P,Pcat=Pcat,M=M,
                                 s=S[t,],seed=seed+seed*1000+t-1
                                 #,pNAs=pNAs,typeNA=0
    )
    
    temp=data.frame(simDat#$SimData
    )
    temp$m=1:M
    temp$t=rep(t-1,M)
    Y=rbind(Y,temp)
    
    # temp=data.frame(simDat$SimData.NA)
    # temp$m=1:M
    # temp$t=rep(t-1,M)
    #Y.NA=rbind(Y.NA,temp)
    
    S[t,]=order_states_condMean(Y[Y$t==(t-1),dim(Y)[2]-2],S[t,])
    
  }
  
  data=data[-1,]
  S=S[-1,]
  Y=Y[-which(Y$t==0),]
  #Y.NA=Y.NA[-which(Y.NA$t==0),]
  
  Y <- Y %>% relocate(t,m)
  #Y.NA <- Y.NA %>% relocate(t,m)
  
  Y.NA=apply(Y[,-(1:2)],2,punct,pNAs=pNAs,type=0)
  Y.NA=as.data.frame(Y.NA)
  Y.NA[,1:Pcat]=Y.NA[,1:Pcat]%>%mutate_all(as.factor)
  Y.NA[,-(1:Pcat)]=Y.NA[,-(1:Pcat)]%>%mutate_all(as.numeric)
  Y.NA=data.frame(t=Y$t,m=Y$m,Y.NA)
  
  if(pGap>0){
    set.seed(seed)
    gaps=sort(sample(1:TT,round(TT*pGap),replace=F))
    Y=Y[-which(Y$t %in% gaps),]
    Y.NA=Y.NA[-which(Y.NA$t %in% gaps),]
  }
  
  return(list(S = S, 
              Y=Y,
              Y.NA=Y.NA,
              spatial_points = spatial_points,
              spatial_cov = spatial_cov,
              dist_matrix = dist_matrix)
  )
}

dist_fun_exp_norm=function(D){
  
  dist_weights=exp(D)
  for(m in 1:nrow(D)){
    dist_weights[m,]=ifelse(D[m,] == 0, 0, exp(D[m,]))/sum(ifelse(D[m,] == 0, 0, exp(D[m,]))) 
  }
  
  return(dist_weights)
  
}

dist_fun_norm=function(D){
  
  dist_weights=D
  for(m in 1:nrow(D)){
    dist_weights[m,]=ifelse(D[m,] == 0, 0, D[m,])/sum(ifelse(D[m,] == 0, 0, D[m,])) 
  }
  return(dist_weights)
  
}

fuzzy_STJM=function(Y,
                    K,
                    dist_weights,
                    lambda,
                    gamma,
                    n_init=10,
                    max_iter=10,
                    tol=NULL,
                    verbose=F,
                    ncores_M=NULL){
  
  # dist_weights is a matrix with the weights for the distance between the spatial points
  
  Y=Y[order(Y$t,Y$m),]
  P=ncol(Y)-2
  TT=length(unique(Y$t))
  M=length(unique(Y$m))
  best_loss <- NULL
  best_S <- NULL
  
  Y.orig=Y
  
  Y=subset(Y,select=-c(t,m))
  
  # Scale numeric variables
  Y=Y%>%mutate_if(is.numeric,function(x)as.numeric(scale(x)))
  
  Y <- data.frame(t=Y.orig$t,m=Y.orig$m,Y)
  
  # Reorder columns so that we have t,m, cat vars and cont vars
  cat.indx=which(sapply(Y, is.factor))
  cat.flag=T
  if(length(cat.indx)==0){
    cat.flag=F
  }
  cont.indx=which(sapply(Y, is.numeric))
  cont.indx=cont.indx[-(1:2)]
  Y=Y[,c(1,2,cat.indx,cont.indx)]
  
  YY=subset(Y,select=-c(t,m))
  
  cont.indx=cont.indx-2
  cat.indx=cat.indx-2
  
  Ycont=YY[,cont.indx]
  
  Ycat=YY[,cat.indx]
  levels_cat=lapply(Ycat,levels)
  names(levels_cat)=cat.indx
  
  n_cat=length(cat.indx)
  n_cont=length(cont.indx)
  
  # Missing data imputation 
  # TO DO
  # Mcont=ifelse(is.na(Ycont),T,F)
  # Mcat=ifelse(is.na(Ycat),T,F)
  # mu <- apply(Ycont, 2, median, na.rm = TRUE)
  # #mu <- colMeans(Ycont,na.rm = T)
  # mo <- apply(Ycat,2,Mode)
  # for(i in 1:n_cont){
  #   Ycont[,i]=ifelse(Mcont[,i],mu[i],Ycont[,i])
  # }
  # 
  # if(cat.flag){
  #   for(i in 1:n_cat){
  #     x=Ycat[,i]
  #     Ycat[which(is.na(Ycat[,i])),i]=mo[i]
  #   }
  # }
  # 
  # YY[,cont.indx]=Ycont
  # YY[,cat.indx]=Ycat
  # 
  # Y[,-(1:2)]=YY
  
  for (init in 1:n_init) {
    
    loss_old <- 1e10
    
    # State initialization
    S=matrix(0,nrow=TT,ncol=M)
    for(m in 1:M){
      S[,m]=initialize_states(Y[which(Y$m==m),-(1:2)],K)
    }
    vec_S=as.vector(t(S))
    
    SS <- matrix(0, nrow = TT * M, ncol = 2 + K)
    SS[, 1] <- rep(1:TT, each = M)  # Time indices
    SS[, 2] <- rep(1:M, times = TT) # Spatial indices
    
    # Compute row indices in SS corresponding to (t, m)
    row_indices <- rep(1:(TT * M))  # Row positions in SS
    col_indices <- vec_S + 2  # Convert S into a vector and shift for SS indexing
    
    # Assign 1s in a single step
    SS[cbind(row_indices, col_indices)] <- 1 
    
    # Convert to DataFrame and set column names
    SS <- data.frame(SS)
    colnames(SS) <- c("t", "m", 1:K)
    
    SS_old=SS
    
    mu <- matrix(0, nrow=K, ncol=length(cont.indx))
    mo <- matrix(0, nrow=K, ncol=length(cat.indx))
    
    for (i in unique(vec_S)) {
      # substitute with medians
      mu[i,] <- apply(Ycont[vec_S==i,], 2, median, na.rm = TRUE)
      mo[i,]=apply(Ycat[vec_S==i,],2,Mode)
    }
    
    mu=data.frame(mu)
    mo=data.frame(mo,stringsAsFactors=TRUE)
    for(i in 1:n_cat){
      mo[,i]=factor(mo[,i],levels=levels(Ycat[,i]))
    }
    mumo=data.frame(matrix(0,nrow=K,ncol=P))
    mumo[,cat.indx]=mo
    mumo[,cont.indx]=mu
    colnames(mumo)=colnames(YY)
    
    for (it in 1:max_iter) {
      
      # E Step
      g=gower.dist(YY,mumo)
      
      if(is.null(ncores_M)){
        for (m in 1:M){
          indx=which(Y$m==m)
          omega_m=dist_weights[m,]
          Wm=sum(omega_m)
          
          g2_1=1/(g[indx,]^2+gamma*Wm+lambda)
          g2_1[1,]=1/(g[indx[1],]^2+gamma*Wm)
          
          s_til_m=g2_1/rowSums(g2_1)
          
          s_i=as.matrix(SS[SS$t==1,-(1:2)])
          
          A=sum((omega_m%*%s_i)/g2_1[1,])
          B=(omega_m%*%s_i)/g2_1[1,]
          
          SS[indx[1],-(1:2)]=s_til_m[1,]-gamma*s_til_m[1,]*A+gamma*B
          
          for(t in 2:TT){
            s_i=as.matrix(SS[SS$t==t,-(1:2)])
            A=sum((omega_m%*%s_i)/g2_1[t,])
            B=(omega_m%*%s_i)/g2_1[t,]
            SS[indx[t],-(1:2)]=
              s_til_m[t,]-
              gamma*(B-s_til_m[t,]*A)+
              lambda*(SS[indx[t-1],-(1:2)]/g2_1[t,]-
                        s_til_m[t,]*sum(SS[indx[t-1],-(1:2)]/g2_1))
            # Potrebbero non sommare a  uno, verificare
            # Intanto normalizzo
            SS[indx[t],-(1:2)]=SS[indx[t],-(1:2)]/sum(SS[indx[t],-(1:2)])
          }
        }
      }
      else{
        num_cores <- ncores_M  
        cl <- makeCluster(num_cores)
        registerDoParallel(cl)
        # SS <- foreach(m = 1:M
        #               #, .combine = '+'
        #               ) %dopar% {
        #   indx=which(Y$m==m)
        #   omega_m=dist_weights[m,]
        #   Wm=sum(omega_m)
        #   
        #   g2_1=1/(g[indx,]^2+gamma*Wm+lambda)
        #   g2_1[1,]=1/(g[indx[1],]^2+gamma*Wm)
        #   
        #   s_til_m=g2_1/rowSums(g2_1)
        #   
        #   s_i=as.matrix(SS[SS$t==1,-(1:2)])
        #   
        #   A=sum((omega_m%*%s_i)/g2_1[1,])
        #   B=(omega_m%*%s_i)/g2_1[1,]
        #   
        #   SS[indx[1],-(1:2)]=s_til_m[1,]-gamma*s_til_m[1,]*A+gamma*B
        #   
        #   for(t in 2:TT){
        #     s_i=as.matrix(SS[SS$t==t,-(1:2)])
        #     A=sum((omega_m%*%s_i)/g2_1[t,])
        #     B=(omega_m%*%s_i)/g2_1[t,]
        #     SS[indx[t],-(1:2)]=
        #       s_til_m[t,]-
        #       gamma*(B-s_til_m[t,]*A)+
        #       lambda*(SS[indx[t-1],-(1:2)]/g2_1[t,]-
        #                 s_til_m[t,]*sum(SS[indx[t-1],-(1:2)]/g2_1))
        #     # Potrebbero non sommare a  uno, verificare
        #     # Intanto normalizzo
        #     SS[indx[t],-(1:2)]=SS[indx[t],-(1:2)]/sum(SS[indx[t],-(1:2)])
        #   }
        #   return(as.matrix(SS))
        # }
        SS <- foreach(m = 1:M, .combine = rbind, 
                             .packages = "matrixStats") %dopar% {
          indx <- which(Y$m == m)
          omega_m <- dist_weights[m, ]
          Wm <- sum(omega_m)
          
          g2_1 <- 1 / (g[indx, ]^2 + gamma * Wm + lambda)
          g2_1[1, ] <- 1 / (g[indx[1], ]^2 + gamma * Wm)
          
          s_til_m <- g2_1 / rowSums(g2_1)
          
          # Create a local copy of SS for this iteration
          SS_local <- SS  # Assuming SS is pre-initialized and accessible
          
          s_i <- as.matrix(SS_local[SS_local$t == 1, -(1:2)])
          
          A <- sum((omega_m %*% s_i) / g2_1[1, ])
          B <- (omega_m %*% s_i) / g2_1[1, ]
          
          SS_local[indx[1], -(1:2)] <- s_til_m[1, ] - gamma * s_til_m[1, ] * A + gamma * B
          
          for (t in 2:TT) {
            s_i <- as.matrix(SS_local[SS_local$t == t, -(1:2)])
            A <- sum((omega_m %*% s_i) / g2_1[t, ])
            B <- (omega_m %*% s_i) / g2_1[t, ]
            
            SS_local[indx[t], -(1:2)] <-
              s_til_m[t, ] -
              gamma * (B - s_til_m[t, ] * A) +
              lambda * (SS_local[indx[t - 1], -(1:2)] / g2_1[t, ] -
                          s_til_m[t, ] * sum(SS_local[indx[t - 1], -(1:2)] / g2_1))
            
            # Normalize
            SS_local[indx[t], -(1:2)] <- SS_local[indx[t], -(1:2)] / sum(SS_local[indx[t], -(1:2)])
          }
          
          # Return the data frame for this m
          return(data.frame(t = SS_local$t[indx], m = m, 
                            SS_local[indx, -(1:2)]))
        }
        stopCluster(cl)
        colnames(SS) <- c("t", "m", 1:K)
        # sort SS by t 
        SS=SS[order(SS$t), ]
        
        }
      
      
      # M Step
      for(k in 1:K){
        
        mu[k,]=apply(Ycont,2,function(x){poliscidata::wtd.median(x,weights=SS[,k+2]^2)})
        mo[k,]=apply(Ycat,2,function(x){poliscidata::wtd.mode(x,weights=SS[,k+2]^2)})
        # mu[k,]=apply(Ycont, 2, function(x) weighted_median(x, weights = SS[,k+2]))
        # mo[k,]=apply(Ycat,2,function(x)weighted_mode(x,weights=SS[,k+2]))
      }
      
      mumo[,cat.indx]=mo
      mumo[,cont.indx]=mu
      colnames(mumo)=colnames(YY)
      
      # Re-fill-in missings 
      # TO DO
      
      # Compute objective function
      g=gower.dist(YY,mumo)
      # sum_result <- 0
      # for (t in 1:TT) {
      #   for (m in 1:M) {
      #     s_t_m <- SS[SS$t==t&SS$m==m,-(1:2),]
      # 
      #     for (i in 1:M) {
      #       if (i != m) {
      #         s_t_i <- SS[SS$t==t&SS$m==i,-(1:2),]
      # 
      #         diff_norm_sq <- sum((s_t_i - s_t_m)^2)
      #         sum_result <- sum_result + dist_weights[m, i] * diff_norm_sq
      # 
      #       }
      #     }
      #   }
      # }
      
      value_opt=sum(g^2)+
        lambda*sum((SS[-which(SS$t==1),-(1:2)]-SS[-which(SS$t==TT),-(1:2)])^2)
      #+gamma*sum_result
      
      if (verbose) {
        cat("Init.: ", init, "// Iteration: ", it, "// Obj. Fun.: ", value_opt, "\n")
      }
      
      if (!is.null(tol)) {
        if (sum((SS-SS_old)^2)<tol) {
          break
        }
        else if( (loss_old - value_opt) < tol){
          break
        }
      } 
      
      SS_old=SS
      
      loss_old <- value_opt
    }
    if (is.null(best_S)||(loss_old < best_loss)
        ) {
      best_loss <- loss_old
      best_S <- SS
    }
    
  }
  
  return(list(S=best_S))
  
}


simstud_fuzzySTJM=function(lambda,gamma,seed,M,TT,beta, theta,
                             mu=.5,rho=0.2,
                             K=3,P=20,phi=.8,Pcat=NULL,pNAs=0,pg=0,
                           ncores_M=3){
  
  library(MASS)
  if(is.null(Pcat)){
    Pcat=floor(P/2)
  }
  
  result <- generate_spatio_temporal_data(M, TT, theta, beta, K = K,
                                          mu=mu,rho=rho,phi=phi,
                                          P=P,Pcat=Pcat,seed=seed,pGap=pg,pNAs=pNAs)
  
  Y.compl=result$Y
  D=result$dist_matrix
  Y=result$Y.NA
  
  #dist_weights=dist_fun_norm(D)
  dist_weights=dist_fun_exp_norm(D)
  
  # tf=I(pg>0)
  prova=fuzzy_STJM(Y,K,dist_weights,
                   lambda=lambda,gamma=gamma,
                   verbose=F,tol=1e-6,
                   ncores_M=ncores_M,
                   n_init = 5)
  
  best_S=prova$S
  
  MAP=data.frame(t=Y$t,m=Y$m,
    MAP=apply(best_S[,-c(1,2)],1,which.max))
  MAP$MAP=factor(MAP$MAP,levels=1:K)
  
  df <- as.data.frame(result$S)
  colnames(df) <- paste0("m", 1:ncol(df))
  df$t <- 1:nrow(df)
  df_long <- tidyr::pivot_longer(df, cols = starts_with("m"), 
                                 names_to = "m", 
                                 values_to = "true_state")
  df_long$m <- as.numeric(gsub("m", "", df_long$m))
  df_long=as.data.frame(df_long)
  df_long$true_state=factor(df_long$true_state,levels=1:K)
  
  # TY=unique(Y$t)
  # S_true=result$S[TY,]
  # 
  # for(t in 1:length(TY)){
  #   best_s[t,]=order_states_condMean(Y[Y$t==TY[t],dim(Y)[2]],best_s[t,])
  # }
  # 
  ARI=adj.rand.index(df_long$true_state,MAP$MAP)
  BAC=caret::confusionMatrix(MAP$MAP,df_long$true_state)$overall[1]
  
  return(list(ARI=ARI,
              BAC=BAC,
              S_true=df_long,
              best_S=best_S,
              seed=seed,
              M=M,TT=TT,P=P,
              lambda=lambda,
              gamma=gamma
              # ,
              # mu=mu,rho=rho,
              # K=K,P=P,phi=phi,Pcat=Pcat,pNAs=pNAs,pg=pg
              ))
}
